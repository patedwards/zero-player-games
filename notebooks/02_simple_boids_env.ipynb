{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab18d3f7-fd86-4c67-bbbf-3d97072695d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "source": [
    "```\n",
    "!pip3.10 install swig\n",
    "!pip3.10 install gym[box2d]\n",
    "!pip3.10 install imageio imageio[ffmpeg]\n",
    "!pip3.10 install stable-baselines3[extra]\n",
    "!pip3.10 install -e ../\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add4a428-26d4-46d9-9122-061eee0d5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import Video\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\" # this stops pygame opening it's own window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ee2688-5249-494d-a846-cfa7804ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcc665a-0982-4d94-a91e-8d8975a4ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from environments.model_ready_boids import RavenChasingBoids\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58380428-c5fc-4b69-963f-4c478f567faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# Now import the function\n",
    "import gym_utils\n",
    "import controllers\n",
    "import agents\n",
    "import runners\n",
    "import environments\n",
    "from environments.simple_boids import PureBoinds, BoidsWithRavenoid\n",
    "from gym_utils.vis import create_environment_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e66d6f-8048-4ab2-ac7e-0b7359c69f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa0945-2f73-4def-a9a4-fe42e56bf883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = PureBoinds(100, render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "# Create a video\n",
    "video_filename = 'boids.mp4'\n",
    "create_environment_video(env, video_filename, steps=200)\n",
    "\n",
    "# Display the video\n",
    "Video(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bd3d44b-73d3-43fe-b828-ab7eafad87e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a video\u001b[39;00m\n\u001b[1;32m      6\u001b[0m video_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboids.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcreate_environment_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Display the video\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Video(video_filename)\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/src/gym_utils/vis.py:6\u001b[0m, in \u001b[0;36mcreate_environment_video\u001b[0;34m(env, filename, steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_environment_video\u001b[39m(env, filename, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m imageio\u001b[38;5;241m.\u001b[39mget_writer(filename, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m video:\n\u001b[0;32m----> 6\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m      8\u001b[0m             frame \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/gymnasium/wrappers/frame_stack.py:196\u001b[0m, in \u001b[0;36mFrameStack.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    194\u001b[0m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(obs) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stack)]\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m, info\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/gymnasium/wrappers/frame_stack.py:168\u001b[0m, in \u001b[0;36mFrameStack.observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the wrappers current frames to lazy frames.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    :class:`LazyFrames` object for the wrapper's frame buffer,  :attr:`self.frames`\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stack, (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stack)\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLazyFrames\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlz4_compress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/gymnasium/wrappers/frame_stack.py:33\u001b[0m, in \u001b[0;36mLazyFrames.__init__\u001b[0;34m(self, frames, lz4_compress)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, frames: \u001b[38;5;28mlist\u001b[39m, lz4_compress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lazyframe for a set of frames and if to apply lz4.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m        DependencyNotInstalled: lz4 is not installed\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(frames),) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_shape\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m frames[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env = BoidsWithRavenoid(100, render_mode=\"rgb_array\")\n",
    "from gymnasium.wrappers import FrameStack\n",
    "env = FrameStack(env, num_stack=7)\n",
    "\n",
    "# Create a video\n",
    "video_filename = 'boids.mp4'\n",
    "create_environment_video(env, video_filename, steps=200)\n",
    "\n",
    "# Display the video\n",
    "Video(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd48c1d2-7d9e-4e6b-835b-8e39736138d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0 steps taken. Average steps per episode: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"mock_boids.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from environments.model_ready_boids import RavenChasingBoids\n",
    "\n",
    "class AlternatingActionModel:\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.step_count = 0\n",
    "        self.env = env\n",
    "\n",
    "    def predict(self, observation, state=None, episode_start=None, deterministic=None):\n",
    "        # Define the policy: alternate between action 1 and action 2 every 4 steps\n",
    "        action_period = 4\n",
    "        if (self.step_count // action_period) % 2 == 0:\n",
    "            action = 1  # Use action 1 for 4 steps\n",
    "        else:\n",
    "            action = 2  # Then switch to action 2 for the next 4 steps\n",
    "\n",
    "        self.step_count += 1\n",
    "        return action, state\n",
    "\n",
    "    def get_env(self): return self.env\n",
    "        \n",
    "env = RavenChasingBoids(10, render_mode=\"rgb_array\")\n",
    "print(env.history)\n",
    "model = AlternatingActionModel(env)\n",
    "\n",
    "from gym_utils.vis import create_environment_video_with_model\n",
    "\n",
    "video_filename = 'mock_boids.mp4'\n",
    "create_environment_video_with_model(model, video_filename, steps=1000)\n",
    "print(env.history)\n",
    "Video(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85d08b-89a2-4dce-84fe-e11d74d06347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82f12174-8c14-46fc-a95a-6240a80ad508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAEAAQADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigDc8HarfaL4x0q/025tLW7S4VEmvOIFD/ACN5h7JtY5I5AyRg817f8f8Aw1q+u6p4bGn3Ed08zva22lqAJTIfmeUHum1VDE4CYU/xHHzpXWeEfiDrXhPxLaaws8l8ILcWbQXMhYG2yD5Sk5KAEAjHAI6EZBAOf1XSr7Q9UuNM1O2ktry3fZLE/VT/ACIIwQRwQQRkGqddJ458Z33jrxLJrF9HHCAght4E5EMQJIXOMscsSSepJwAMAc3QAUUUUAFFFFABRRRQAUUUUAWLC1+3ajbWf2iC38+VIvOuH2Rx7iBuduyjOSewr0j4q/Cqz+H+naVeWesfaftH7maGfCyNIBlpI1H/ACz6Ag5Kkry27jy+rF1f3l95H2y7nuPIiWCHzpC/lxr91Fz0UZOAOBQBXooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiirFjYXmp3kdnYWk93dSZ2QwRmR2wCThRycAE/hQBXooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD1j4QeNvB3hXTtbh8Radm6uIm23Hled9oiwAbbaeFyeecK2fmI2rnP+G3xH0vwP4t1S+k0PGmX+4IkJ8ye0QMWVEdiNy8gNkjO1Wz8uD5vRQBseKte/4SfxRqOtfYoLL7ZKZPIgHyrxjk92OMs3GWJOBnFY9FFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUV2GvfDHxT4b8L2fiDUbHy7S4/1iAkyWuT8vmrj5d3brg4DbWIFcfQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABViwuvsOo2159nguPIlSXybhN8cm0g7XXupxgjuKr0UAesfE/4x/8JxodnpGm2U9janbNe+ZJy8gz+7G04aMHnLDJIU4Xbz5PRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV6R/wprXf+Fa/wDCX/aIN3lfa/sPG77Lt3eZvzjdj5tn93vu+WvN62P+Eq13/hF/+Ea/tOf+xvN877Jkbd2c4z125+bbnbu+bGeaAMeiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAortPiN8Ob74eapbQT3Ud5Z3aFra5VdhcrjerJklSCw7kEEc5yBxdABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFfQ/gDxN8LvC3w1S8mMDahNE9vqEM8Ky3c7lcvGFx/qW2gLnCdNxDbqANzxL4d8M/F34fQa7Y6pHHfWduQupXe2NkKDLx3IUAKOpJAwudy5U4b5cqxDf3lvZ3NnDdzx2t1t+0QpIQku05Xco4bB5GeleseBP8AhV3/AAq/V/8AhI/+Qvz9o37ftPU+V9k/TPvnf8mKAPH6KKACSABkntQlfRAFFSy28kIUuOD3Hb2qKta1CpQm6dWLT7MSaeqCiiishhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUAkEEHBHeiihO2qAlluJJgoc8DsO/vUVFFa1q9SvN1Ksm33YkktEFFFFZDCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACirFh9j/tG2/tHz/sPmp9o+z48zy8jdszxuxnGeM16h8X/+Fdf2don/AAh/kfbvKXd9i/1f2fBx52efOzjr8+N2/wDhoA8nooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoor2D/hBPA3/AAo3/hI/7b/4m/3vtGT/AMfG3/jz8rP69f487OKAPH6KKKACiiigAooooAKKKKACiiigAr0j4NeFPDXi3xRPa+IbnLRxbrbT9zR/ajg7jvBB+QDO0EE5z0VgfN6KANjxVp+l6V4o1Gx0XUf7R02GUrBdY++McjI4bByu4cNjI4IrHoooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACirFhDb3Go20N5dfZLWSVEmuPLMnlISAz7Ry2Bk4HXFeofF/wT4O8K6dok3h3Uc3VxEu6383zvtEWCRc7hwuTxxhWz8oG1sgHk9FFFAG5/xJ/+Ed/6ev8AyJ5n/wAT+mPesOiis4Q5L63udeKxX1jk9xR5UlorXt1fmFFFFaHIFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAAG30lEQVR4Ae3d63qjIBAA0Nhv3/+Vs3TTnaRjG6NBRTz9U4iKzIGJ5D5c/M0UuF6v6YhhGNItqkcR+DhKR/WTwBoCEmC2arq/T9XZzTlgVwEJsITfpH9FrawVx8vFVw7cch8J8K52+2P8boSLjg+WKCxqZvWDJMAS4jSoqbqkxb6OSSCp2lSsEqDmcJSRbnmwa4baS1sSYMlIpscAt2pM/SgsabqLY370aTMyCbBwXGKM0+y/NScHks9C5fUP8wpOHePxjI8ZUOcEWllHwBWgjmua7qla5xxaIdC4QLkOjC8FjfdZ9wgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBA4k4AezDjRYp+3qu78S+dssj9+Ki8JpiQXessBbCRCTOwq3UJ9XW+bQt7MJLE8As/xsc6XLeJcnwBOO9DPRqfrkQJsIbCywPAHStP6tmm7fODynI7CuQFkIpbXQuufTOgECBAgQIECAAAECBAgQIECAAAECBAgQIECAQNMC7bx8tPyV4KaBda5hgXjlNAo7dlYC7Ih/xlOnSZ+q24tIgO3NnbEhAQnQ0GB035Xx/f3u75WUAN3POgE+E5AAz3RsqyuQ7u9Tte65tEagUYGyEBqvhRrtq24RIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwH+B67+//zX/Oxf46Dy+meGVyX87IgozG7D7wQQkwH3A0qRP1ft+Sh0JSICOBlMo8wUkwN1sGIZ75XJJ1cdNygS6FSgrH4ufbkdXYAQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEDi4QLzp/dtHQA4elO4TmBZIH/bwibBpMnt0I5Bmf4lLAnQzuAKZEBjP/s8EiMXQxNE2E+hR4OsK8GNy9BivmM4rMP6Wg3LLfQkkB847NU4TeeRAKdzKf04Tu0AJfArc5n1Y3K8AaUPsoUCgY4F7AlgCdTzMQvtN4OPxjl8O/Mbk9l4FPp8GfYwtVR83KRPoT+C+BIrY5EBQKHQv8G0JFNHKgaBQ6Fvg8wrw+DCg72hFRyAJfC2BUg6kajpGlUCfAmXlY/HT59CK6h0BifGOnmNbFvjhWaDU3bgmRCHtoErguAITCZAmfaoeN2w9J3ATmEgATAT6FphIgPR0UKq+TFNebP72evPLB9qRwLoCEwlQTh6TPgozexRTPwozG7A7gdUE1v5WiPGkX/uMq1FpuEeB6StAj1GLicCXwNoJkO7vU9UwEDiFQFkIjddCp4hckAQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQ6F7j+++s8SOH1JfBRK5wy+W9NRaFWy9ohsJ5AnQRIkz5V1+u9lgm8KVAnAd7shMMJ7CVQJwGGYXgMIFUfNykT6FagrHwsfrodXYERIECAAAECBAgQIPCmgIdMbwJWObzOs0BVunKqRuLZgiicKvx2gpUAO4xFmvSpukOHTnxKCXDiwRf65SIBdpgF6YXCVN2hQ05JYHuBsvKx+Nme3RkJECBAgMBKAq7sK8Fqdg2Byg+CY1EbhTU6rU0CtQRqJkCa9Klaq8faIVBRoGYCVOyWpghsI1AzAdLz2am6TTzOQmBngbLysfjZeQycngABAgSeCZRvcfn6Ipdne9nWuUDNxwAHooqpH4UDdV5XKwqcMQHSpE/Viriaal/gjAnQ/qjo4WYCZ0yAb99hdLmk6mb0TkRgT4Gy8rH42XMAnJsAAQIECBAgQIDApIA3WUwS2WGuwGGeBYr3F0Vhbqj2JzAWOEYCpEmfquOo3ELgRYFjJMCLwdiNwFyBYyRA+mhBqs6N2f4EDilQVj4WP4ccuYY7/ReyfknpgIGS1gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "env = RavenChasingBoids(20, render_mode=\"rgb_array\")\n",
    "env.step(0)\n",
    "for i in range(100):\n",
    "    env.step(0)\n",
    "arr = env.render()\n",
    "Image.fromarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a9ec654-82ae-48bc-9baa-47a2e1d5a147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABgAGADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAqx9gvP7O/tH7JP9h83yPtPlny/Mxu2bum7HOOuKLCa3t9Rtpry1+12scqPNb+YY/NQEFk3DlcjIyOma+l5/HPgr4kRad8PtNivktdTiTe0EawCzWJTKIwCCCwMSrgArhuG4xQB8wUVseKtB/wCEY8Uajov22C9+xymPz4D8rcZ5HZhnDLzhgRk4zWPQAUUUUAFFFFABRRRQAUUUUAFdB4H1+38L+NNL1q7gnnt7WXdIkEpjcggjIIIzjOdpOGxtPBNc/RQB7/8AHDT/AAdrPhey8aafqMCald7VtzCM/wBoICAwZeoaMdWPTGxuduPAKK9g8d+O/A2t/C/SNG0bRPI1CHHlQ4I/s3BG/wCfH7zfz/vZ3NhgBQB4/RRUkMLTyBF47k+grSlSnWmqdNXb0SE2krsjoqSaFoJCjc9wfUVHRVpTozdOorNboE01dBRRRWYwooooAKKKKACiiigApVZkYMpwR0NJRTjJxalF2aAVmZ2LMck9TSUUUSk5Nyk7tgFFFFIAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAAABf0lEQVR4Ae3Z4W6DIBAA4Lr0/V95I+1ycYBEFhsVPn8dcDHweWcT+3i4CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEphD4fl1THLV2yKU2+TuXZNary9JKXmeOFH9tHSbT2Uobfr4ORCcefB0oltfB6130p+nWq6PGdaDydRMzsxVXHSiVQ4ikIOJ3mUxl9Gy0RubSyBx4abOCGmeeCu4/QFO1WAfQunDmMeoAylCyYaMlb73UAVSecwajDqB1iwXW8EYdQAmlahRYQwZ9QKXRhGS76iB11vDNtQuiN2k8uO4Wa5BFTUXQSL7L0mFAGUo2vAtHuc/DgMpbjzFzGFD2c5YNd2OlD3LX+iZ3GFAiCJQIdru8E4Mmgs4bfCD9On9UlCiX2NuRFfSB53f+La8DlNVLNjxf6iI7SI1W9tpF9mYbBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAicKvADvKpMGfZI34EAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=96x96>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(env._get_obs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1aa2a81-ab33-4780-8339-7a2da84ea5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps taken. Average steps per episode: nan\n",
      "0 steps taken. Average steps per episode: nan\n",
      "1 steps taken. Average steps per episode: nan\n"
     ]
    }
   ],
   "source": [
    "from environments.model_ready_boids import RavenChasingBoids\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = RavenChasingBoids(20, render_mode=\"rgb_array\")\n",
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07246e-82fc-4664-a7c4-736ecaf5154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wow, that passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "454443f9-ddcd-4607-aa66-35f3b609dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FrameStack\n",
    "env = FrameStack(env, num_stack=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46089620-6efa-4cc7-a129-bd193f509524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a112ab82-60ca-4cc0-873d-f2c0b2e046ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = RavenChasingBoids(20, render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94b24f91-7cc5-4f12-bc20-b6ff993c6e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "0 steps taken. Average steps per episode: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:231: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 55.31GB > 7.73GB\n",
      "  warnings.warn(\n",
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 steps taken. Average steps per episode: 1001.0\n",
      "806 steps taken. Average steps per episode: 903.5\n",
      "1001 steps taken. Average steps per episode: 936.0\n",
      "359 steps taken. Average steps per episode: 791.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 792      |\n",
      "|    ep_rew_mean      | 2.25     |\n",
      "|    exploration_rate | 0.985    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 271      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 3167     |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 833.6\n",
      "490 steps taken. Average steps per episode: 776.3333333333334\n",
      "1001 steps taken. Average steps per episode: 808.4285714285714\n",
      "1001 steps taken. Average steps per episode: 832.5\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 832      |\n",
      "|    ep_rew_mean      | 1.88     |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 273      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 6660     |\n",
      "----------------------------------\n",
      "471 steps taken. Average steps per episode: 792.3333333333334\n",
      "678 steps taken. Average steps per episode: 780.9\n",
      "1001 steps taken. Average steps per episode: 800.9090909090909\n",
      "1001 steps taken. Average steps per episode: 817.5833333333334\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 818      |\n",
      "|    ep_rew_mean      | 1.83     |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 249      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 9811     |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 831.6923076923077\n",
      "1001 steps taken. Average steps per episode: 843.7857142857143\n",
      "1001 steps taken. Average steps per episode: 854.2666666666667\n",
      "1001 steps taken. Average steps per episode: 863.4375\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 863      |\n",
      "|    ep_rew_mean      | 1.62     |\n",
      "|    exploration_rate | 0.934    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 13815    |\n",
      "----------------------------------\n",
      "643 steps taken. Average steps per episode: 850.4705882352941\n",
      "558 steps taken. Average steps per episode: 834.2222222222222\n",
      "1001 steps taken. Average steps per episode: 843.0\n",
      "470 steps taken. Average steps per episode: 824.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 824      |\n",
      "|    ep_rew_mean      | 1.8      |\n",
      "|    exploration_rate | 0.922    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 250      |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 16487    |\n",
      "----------------------------------\n",
      "784 steps taken. Average steps per episode: 822.4285714285714\n",
      "1001 steps taken. Average steps per episode: 830.5454545454545\n",
      "1001 steps taken. Average steps per episode: 837.9565217391304\n",
      "935 steps taken. Average steps per episode: 842.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 842      |\n",
      "|    ep_rew_mean      | 1.79     |\n",
      "|    exploration_rate | 0.904    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 20208    |\n",
      "----------------------------------\n",
      "402 steps taken. Average steps per episode: 824.4\n",
      "1001 steps taken. Average steps per episode: 831.1923076923077\n",
      "1001 steps taken. Average steps per episode: 837.4814814814815\n",
      "676 steps taken. Average steps per episode: 831.7142857142857\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 832      |\n",
      "|    ep_rew_mean      | 1.89     |\n",
      "|    exploration_rate | 0.889    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 230      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 23288    |\n",
      "----------------------------------\n",
      "371 steps taken. Average steps per episode: 815.8275862068965\n",
      "1001 steps taken. Average steps per episode: 822.0\n",
      "663 steps taken. Average steps per episode: 816.8709677419355\n",
      "264 steps taken. Average steps per episode: 799.59375\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 800      |\n",
      "|    ep_rew_mean      | 1.94     |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 25587    |\n",
      "----------------------------------\n",
      "580 steps taken. Average steps per episode: 792.939393939394\n",
      "1001 steps taken. Average steps per episode: 799.0588235294117\n",
      "488 steps taken. Average steps per episode: 790.1714285714286\n",
      "552 steps taken. Average steps per episode: 783.5555555555555\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 784      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.866    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 232      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 28208    |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 789.4324324324324\n",
      "523 steps taken. Average steps per episode: 782.421052631579\n",
      "1001 steps taken. Average steps per episode: 788.025641025641\n",
      "200 steps taken. Average steps per episode: 773.325\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 773      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 233      |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 30933    |\n",
      "----------------------------------\n",
      "787 steps taken. Average steps per episode: 773.6585365853658\n",
      "1001 steps taken. Average steps per episode: 779.0714285714286\n",
      "410 steps taken. Average steps per episode: 770.4883720930233\n",
      "963 steps taken. Average steps per episode: 774.8636363636364\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 775      |\n",
      "|    ep_rew_mean      | 2.05     |\n",
      "|    exploration_rate | 0.838    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 34094    |\n",
      "----------------------------------\n",
      "196 steps taken. Average steps per episode: 762.0\n",
      "288 steps taken. Average steps per episode: 751.695652173913\n",
      "1001 steps taken. Average steps per episode: 757.0\n",
      "255 steps taken. Average steps per episode: 746.5416666666666\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 747      |\n",
      "|    ep_rew_mean      | 2.08     |\n",
      "|    exploration_rate | 0.83     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 234      |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 35834    |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 751.734693877551\n",
      "1001 steps taken. Average steps per episode: 756.72\n",
      "1001 steps taken. Average steps per episode: 761.5098039215686\n",
      "1001 steps taken. Average steps per episode: 766.1153846153846\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 766      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.811    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 235      |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 39838    |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 770.5471698113207\n",
      "1001 steps taken. Average steps per episode: 774.8148148148148\n",
      "1001 steps taken. Average steps per episode: 778.9272727272727\n",
      "1001 steps taken. Average steps per episode: 782.8928571428571\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 783      |\n",
      "|    ep_rew_mean      | 1.96     |\n",
      "|    exploration_rate | 0.792    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 236      |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total_timesteps  | 43842    |\n",
      "----------------------------------\n",
      "642 steps taken. Average steps per episode: 780.421052631579\n",
      "786 steps taken. Average steps per episode: 780.5172413793103\n",
      "1001 steps taken. Average steps per episode: 784.2542372881356\n",
      "1001 steps taken. Average steps per episode: 787.8666666666667\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 788      |\n",
      "|    ep_rew_mean      | 1.98     |\n",
      "|    exploration_rate | 0.775    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 238      |\n",
      "|    time_elapsed     | 198      |\n",
      "|    total_timesteps  | 47272    |\n",
      "----------------------------------\n",
      "390 steps taken. Average steps per episode: 781.344262295082\n",
      "731 steps taken. Average steps per episode: 780.5322580645161\n",
      "1001 steps taken. Average steps per episode: 784.031746031746\n",
      "1001 steps taken. Average steps per episode: 787.421875\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 787      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.761    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 237      |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 50395    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000114 |\n",
      "|    n_updates        | 98       |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 790.7076923076924\n",
      "466 steps taken. Average steps per episode: 785.7878787878788\n",
      "1001 steps taken. Average steps per episode: 789.0\n",
      "1001 steps taken. Average steps per episode: 792.1176470588235\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 792      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 220      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 53864    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.33e-05 |\n",
      "|    n_updates        | 965      |\n",
      "----------------------------------\n",
      "383 steps taken. Average steps per episode: 786.1884057971015\n",
      "1001 steps taken. Average steps per episode: 789.2571428571429\n",
      "777 steps taken. Average steps per episode: 789.0845070422536\n",
      "1001 steps taken. Average steps per episode: 792.0277777777778\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 792      |\n",
      "|    ep_rew_mean      | 2.01     |\n",
      "|    exploration_rate | 0.729    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 207      |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 57026    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000229 |\n",
      "|    n_updates        | 1756     |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 794.8904109589041\n",
      "1001 steps taken. Average steps per episode: 797.6756756756756\n",
      "995 steps taken. Average steps per episode: 800.3066666666666\n",
      "824 steps taken. Average steps per episode: 800.6184210526316\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 801      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.711    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 195      |\n",
      "|    time_elapsed     | 311      |\n",
      "|    total_timesteps  | 60847    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000104 |\n",
      "|    n_updates        | 2711     |\n",
      "----------------------------------\n",
      "266 steps taken. Average steps per episode: 793.6753246753246\n",
      "784 steps taken. Average steps per episode: 793.5512820512821\n",
      "1001 steps taken. Average steps per episode: 796.1772151898734\n",
      "227 steps taken. Average steps per episode: 789.0625\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 789      |\n",
      "|    ep_rew_mean      | 2.01     |\n",
      "|    exploration_rate | 0.7      |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 189      |\n",
      "|    time_elapsed     | 332      |\n",
      "|    total_timesteps  | 63125    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000846 |\n",
      "|    n_updates        | 3281     |\n",
      "----------------------------------\n",
      "742 steps taken. Average steps per episode: 788.4814814814815\n",
      "527 steps taken. Average steps per episode: 785.2926829268292\n",
      "962 steps taken. Average steps per episode: 787.4216867469879\n",
      "1001 steps taken. Average steps per episode: 789.9642857142857\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 790      |\n",
      "|    ep_rew_mean      | 2.05     |\n",
      "|    exploration_rate | 0.685    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 362      |\n",
      "|    total_timesteps  | 66357    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.63e-05 |\n",
      "|    n_updates        | 4089     |\n",
      "----------------------------------\n",
      "1001 steps taken. Average steps per episode: 792.4470588235295\n",
      "379 steps taken. Average steps per episode: 787.6395348837209\n",
      "1001 steps taken. Average steps per episode: 790.0919540229885\n",
      "1001 steps taken. Average steps per episode: 792.4886363636364\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 792      |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.669    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 175      |\n",
      "|    time_elapsed     | 396      |\n",
      "|    total_timesteps  | 69739    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.13e-05 |\n",
      "|    n_updates        | 4934     |\n",
      "----------------------------------\n",
      "762 steps taken. Average steps per episode: 792.1460674157304\n",
      "1001 steps taken. Average steps per episode: 794.4666666666667\n",
      "1001 steps taken. Average steps per episode: 796.7362637362637\n",
      "755 steps taken. Average steps per episode: 796.2826086956521\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 796      |\n",
      "|    ep_rew_mean      | 1.99     |\n",
      "|    exploration_rate | 0.652    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 169      |\n",
      "|    time_elapsed     | 433      |\n",
      "|    total_timesteps  | 73258    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000699 |\n",
      "|    n_updates        | 5814     |\n",
      "----------------------------------\n",
      "843 steps taken. Average steps per episode: 796.7849462365591\n",
      "659 steps taken. Average steps per episode: 795.3191489361702\n",
      "1001 steps taken. Average steps per episode: 797.4842105263158\n",
      "994 steps taken. Average steps per episode: 799.53125\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 800      |\n",
      "|    ep_rew_mean      | 2.01     |\n",
      "|    exploration_rate | 0.635    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 468      |\n",
      "|    total_timesteps  | 76755    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.91e-05 |\n",
      "|    n_updates        | 6688     |\n",
      "----------------------------------\n",
      "794 steps taken. Average steps per episode: 799.4742268041238\n",
      "1001 steps taken. Average steps per episode: 801.530612244898\n",
      "1001 steps taken. Average steps per episode: 803.5454545454545\n",
      "1001 steps taken. Average steps per episode: 805.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 806      |\n",
      "|    ep_rew_mean      | 2.02     |\n",
      "|    exploration_rate | 0.617    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 157      |\n",
      "|    time_elapsed     | 511      |\n",
      "|    total_timesteps  | 80552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000212 |\n",
      "|    n_updates        | 7637     |\n",
      "----------------------------------\n",
      "338 steps taken. Average steps per episode: 800.8910891089109\n",
      "1001 steps taken. Average steps per episode: 802.8529411764706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the agent and display a progress bar\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2e6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 312\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    541\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    547\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/src/environments/model_ready_boids.py:110\u001b[0m, in \u001b[0;36mRavenChasingBoids.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mget_agents():\n\u001b[0;32m--> 110\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame()\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/src/agents/base.py:97\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_position()\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnearby_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nearby_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m steering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteer()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperform_step_logic(steering)\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/src/agents/base.py:82\u001b[0m, in \u001b[0;36mAgent.get_nearby_agents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nearby_agents\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Get a list of nearby agents.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     nearby_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_nearby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwill_log:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;66;03m# log the report\u001b[39;00m\n\u001b[1;32m     86\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNearby agents: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, nearby_agents)\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/src/controllers/oop.py:44\u001b[0m, in \u001b[0;36mOOPController.find_nearby\u001b[0;34m(self, agent_id)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent not found\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     43\u001b[0m self_hex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_data[agent_id]\u001b[38;5;241m.\u001b[39mh3_index\n\u001b[0;32m---> 44\u001b[0m nearby_hexes \u001b[38;5;241m=\u001b[39m \u001b[43mh3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_ring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_hex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m nearby_hexes\u001b[38;5;241m.\u001b[39madd(self_hex)\n\u001b[1;32m     47\u001b[0m nearby_agents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/h3/api/_api_template.py:313\u001b[0m, in \u001b[0;36m_API_FUNCTIONS.k_ring\u001b[0;34m(self, h, k)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03mReturn unordered set of cells with H3 distance ``<= k`` from ``h``.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03mThat is, the \"filled-in\" disk.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03munordered collection of H3Cell\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m mv \u001b[38;5;241m=\u001b[39m _cy\u001b[38;5;241m.\u001b[39mdisk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_scalar(h), k)\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_out_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/h3/api/basic_str/_binding.py:29\u001b[0m, in \u001b[0;36m_out_unordered\u001b[0;34m(mv)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_out_unordered\u001b[39m(mv):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# todo: should this be an (immutable) frozenset?\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_cy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint2hex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/h3/api/basic_str/_binding.py:29\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_out_unordered\u001b[39m(mv):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# todo: should this be an (immutable) frozenset?\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[43m_cy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint2hex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m mv)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(2e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9beb3b7-76b3-4091-931a-a0f1ae08bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ravenoid_2M_its\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "100d31ef-01cc-4cbd-81dd-6ed4d4730edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "388 steps taken. Average steps per episode: 795.0743538767396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:231: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 55.31GB > 3.95GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 steps taken. Average steps per episode: 795.0365659777425\n",
      "363 steps taken. Average steps per episode: 794.864918553834\n",
      "1001 steps taken. Average steps per episode: 794.9467831612391\n",
      "1001 steps taken. Average steps per episode: 795.0285827709408\n",
      "1001 steps taken. Average steps per episode: 795.1103174603174\n",
      "721 steps taken. Average steps per episode: 795.0809202697342\n",
      "237 steps taken. Average steps per episode: 794.8596352101507\n",
      "1001 steps taken. Average steps per episode: 794.9413396749901\n",
      "1001 steps taken. Average steps per episode: 795.0229793977813\n",
      "635 steps taken. Average steps per episode: 794.9596039603961\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"ravenoid_2M_its\", env=env)\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "982b3e79-edea-4e15-84cc-065c5a01d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 1.0954451150103321\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward, std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4add796-f5e9-4d47-83b4-1a1a029fcf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "0 steps taken. Average steps per episode: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:231: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 55.31GB > 4.45GB\n",
      "  warnings.warn(\n",
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/patrick/Projects/active_projects/zero-player-games/zero-player-games/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304 steps taken. Average steps per episode: 1304.0\n"
     ]
    }
   ],
   "source": [
    "env = RavenChasingBoids(20, render_mode=\"rgb_array\")\n",
    "model = DQN.load(\"../training/model_training_111724_1_cloud\", env=env)\n",
    "\n",
    "video_filename = 'ravenoid.mp4'\n",
    "create_environment_video_with_model(model, video_filename, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "679604e2-9ca5-4f30-9e27-38524a170a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"ravenoid.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c38082-716e-46b0-bc35-7be15fa13414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_utils.vis import create_environment_video_with_model, create_environment_video\n",
    "from stable_baselines3 import DQN\n",
    "from environments.model_ready_boids import RavenChasingBoids\n",
    "from environments.simple_boids import BoidsWithRavenoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65761c96-4a1e-4989-813a-0725deb4699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RavenChasingBoids(20, render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d264d90-4a18-4c66-a5bf-d1cacc7dc158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.n_boids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87b733f-1281-4979-93bd-12b0c3637c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"boids.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = BoidsWithRavenoid(100, render_mode=\"rgb_array\")\n",
    "# Create a video\n",
    "video_filename = 'boids.mp4'\n",
    "create_environment_video(env, video_filename, steps=200)\n",
    "\n",
    "# Display the video\n",
    "Video(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f79a8-c5f6-4ad0-bc50-a6e7c70e4a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-player-games",
   "language": "python",
   "name": "zero-player-games"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
